---
title: "p8105_hw2_fl2715"
author: "Fengwei Lei"
output: github_document
---

## Load Necessary Library
```{r load_libraries}
library(tidyverse)
library(readxl)
```

## Problem 1

```{r}
nyc_df=
  read_csv(
    "./NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
    na = c("NA", ".", "")) |> 
  janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

The above dataset contains information from `NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. It includes variables such as subway line, station name, latitude, longitude, routes served, entry status, exit-only status, vending availability, entrance type, and ADA compliance. Data cleaning steps involved handling missing values, cleaning column names, selecting relevant variables, and converting the entry variable from “YES”/“NO” to a logical format (TRUE/FALSE). The resulting dataset has `r nrow(nyc_df)` rows and `r ncol(nyc_df)` columns.

But these data is not fully “tidy”: both the route number and route should be treated as variables. To make the dataset tidy, we need to reshape the route variables from wide to long format. This restructuring is helpful when analyzing specific routes.

The below code shows that the number of distinct stations identified both by name and line, where we use the `distinct()` fuction.

```{r}
nyc_df |> 
  select(station_name, line) |> 
  distinct()
```

The following code chunk illustrates the number of stations which are ADA compliant. Firstly, we filter the rows where `ada == TRUE`. And we do the similar steps to the above code chunk.

```{r}
nyc_df |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

The following code chunk shows the proportion of station entrances / exits without vending allow entrance. We firstly filter the rows where there is no vending. And since the variable `entry` is a logical variable, we could compute the mean for calculating proportion.

```{r}
nyc_df |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

Finally, we compute the number of distinct stations that serve the A train, and that are ADA compliant. To reformat the data successfully, we firstly convert the variables `route8`, `route9`, `route10` and `route11` into character type, since they were double type before. After that, we tidy the data with converting `route` from wide to long format. And then, we use `filter()`, `select()`, `distinct()` function to find the regarding results (similar to the above steps).
```{r}
nyc_df=
  nyc_df |> 
  mutate(
    route8=as.character(route8),
    route9=as.character(route9),
    route10=as.character(route10),
    route11=as.character(route11))

nyc_df |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()

nyc_df |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

From the above reformat data, we can see that there are **60** distinct stations serve the A train. And **17** Of the stations that serve the A train and are ADA compliant.

## Problem 2

### Read and Clean Mr. Trash Wheel Data

The following code block reads the Mr. Trash Wheel sheet from the Excel file, starting from the second row to skip the header. We use `janitor::clean_names()` to clean the column names. Then, we apply `filter()` to remove rows with missing values in the dumpster column, retaining only those that contain specific dumpster data. Next, we use `mutate()` to round the values in the sports_balls column to the nearest integer and convert them to integer type while adding a source column to indicate that the data comes from “Mr.”

```{r}
mr_trash_wheel=
  read_excel(
    "./202409 Trash Wheel Collection Data.xlsx",
    sheet="Mr. Trash Wheel",
    range="A2:N655") |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    source="Mr"
  )

mr_trash_wheel
```

### Read and Clean Professor Trash Wheel Data

Next code chunk reads the Professor Trash Wheel sheet from the Excel file, also starting from the second row. Apart form the similar steps in above manipulation, in the `mutate()` function, a new column sports_balls is created and set to 0, as this data source does not have corresponding sports ball data. Additionally, the year column is converted to character type to maintain consistency with Mr. Trash Wheel dataset, and a source column is added to indicate that the data comes from “Professor.”

```{r}
pro_trash_wheel=
  read_excel(
    "./202409 Trash Wheel Collection Data.xlsx",
    sheet="Professor Trash Wheel",
    range="A2:M123") |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    sports_balls = 0,
    year = as.character(year),
    source="Professor"
  )

pro_trash_wheel
```

### Read and Clean Gwynnda Trash Wheel Data

Similar steps are applied to Gwynnda Trash Wheel Data for the Gwynnda Trash Wheel sheet from the Excel file.

```{r}
gwy_trash_wheel=
  read_excel(
    "./202409 Trash Wheel Collection Data.xlsx",
    sheet="Gwynnda Trash Wheel",
    range="A2:L266") |> 
  janitor::clean_names() |> 
  filter(!is.na(dumpster)) |> 
  mutate(
    sports_balls = 0,
    year = as.character(year),
    source="Gwynnda"
  )

gwy_trash_wheel
```

### Combine All Cleaned Datasets

Finally, we use the bind_rows() function to combine the three cleaned datasets (mr_trash_wheel, pro_trash_wheel, and gwy_trash_wheel) into a single tidy dataset called trash_tidy. This operation concatenates the rows from each dataset in order, forming a comprehensive table that contains all Trash Wheel data, making it easier for subsequent analysis and processing.

```{r}
trash_tidy=
  bind_rows(mr_trash_wheel, pro_trash_wheel, gwy_trash_wheel)

trash_tidy
```

The combined dataset contains a total of `r nrow(trash_tidy)` observations, representing data collected from three different Trash Wheels: Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda. Key variables in this dataset include date, dumpsters, weight_tons, sports_balls, and source, which provide insights into the amount and type of trash collected. From the available data, the total weight of trash collected by Professor Trash Wheel is **`r sum(trash_tidy$weight_tons[trash_tidy$source == "Professor"], na.rm = TRUE)`** tons. Additionally, during June 2022, Gwynnda collected a total of **`r sum(trash_tidy$cigarette_butts[trash_tidy$source == "Gwynnda" & trash_tidy$month == "June" & trash_tidy$year==2022]) |> as.integer()`** cigarette butts, illustrating the impact of trash collection efforts in the community.

## Problem 3

### Import Datasets

```{r}
bakers_df=
  readr::read_csv(
    "./gbb_datasets/bakers.csv",
    na = c("NA", ".", "")) |>
  janitor::clean_names() |> 
  separate(baker_name, into = c("baker", "baker_last_name"), sep = " ", extra = "merge", fill = "right")

bakes_df=
  readr::read_csv(
    "./gbb_datasets/bakes.csv",
    na = c("NA", ".", "")) |>
  janitor::clean_names()

results_df=
  readr::read_csv(
    "./gbb_datasets/results.csv",
    na = c("NA", ".", ""),
    skip = 2) |>
  janitor::clean_names()
```

### Data Cleaning Process

First, we inspect the data for completeness and correctness by viewing these three individual datasets and checking for missing or inconsistent data. We use `anti_join()` to examine whether there exist any mismatching between the datasets.

- Compare `results_df` and `bakes_df`

```{r}
diff_results_bakes=anti_join(results_df, bakes_df, by=c("series", "episode", "baker"))
diff_results_bakes
```

From the above results, we find that there are `r nrow(diff_results_bakes)` rows, which means that 596 rows in `results_df` but not in `bakes_df` according to variables "series","episode", "baker". But the difference of the rows between `results_df` and `bakes_df` is `r nrow(results_df)` - `r nrow(bakes_df)`= 588, which is not equal to 596. That means there are some tricky rows in `bakes_df` but not in `results_df` for the variables combination of "series","episode", and "baker".

- Compare `bakes_df` and `bakers_df`

```{r}
diff_bakes_bakers=anti_join(bakes_df, bakers_df, by = "baker")
diff_bakes_bakers
```

Interestingly, for the difference of the variable `baker` between `bakes_df` and `bakers_df`, we find there are some stranger names **"Jo"** in `bakes_df`. That might be a wrong recording information. So we need to convert **"Jo"** into **Jo**.

- Compare `results_df` and `bakers_df`

```{r}
diff_results_bakers=anti_join(results_df, bakers_df, by = "baker")
diff_results_bakers
```

The above results shows that there is one baker named "Joanne", who exits in `results_df` but not in `bakers_df`. Besides, we examine that Joanne does not exist in `bakes_df` neither. 
___It is a reasonable explanation that **Joanne** and **Jo** is exactly the same person!___

So we now clean the bakers' names in `bakes_df` and `results_df`!
We convert **"Jo"** into **Jo** in `bakes_df` and **Joanne** into **Jo** in `results_df` for consistency.

```{r}
bakes_df = bakes_df |>
  mutate( baker = str_replace(baker, "\"Jo\"", "Jo"))

results_df= results_df |> 
  mutate(baker=str_replace(baker, "Joanne", "Jo"))
```

Now we check the differences and consistency of these three data sets after modifying the name.

```{r}
diff_results_bakes=anti_join(results_df, bakes_df, by=c("series", "episode", "baker"))
diff_results_bakes

diff_bakes_bakers=anti_join(bakes_df, bakers_df, by = "baker")
diff_bakes_bakers

diff_results_bakers=anti_join(results_df, bakers_df, by = "baker")
diff_results_bakers
```

Now the results are the same as we expected. So we complete checking for completeness and correctness across datasets and move to merging step.

### Merge Datasets

We  merge the datasets into one final, tidy dataset using `left_join()` on the tibble `results_df`, ensuring that all information about bakers, bakes, and results is included.

```{r}
final_df = results_df |> 
  left_join(bakes_df, by = c("series", "episode", "baker"))  |> 
  left_join(bakers_df, by = c("series", "baker"))

final_df = final_df  |> 
  arrange(series, episode, baker)

write_csv(final_df, "./gbb_datasets/final_bakeoff_data.csv")

final_df
```

The final dataset contains information on each baker, the bakes they completed, and their performance across episodes and seasons. The dataset is well-organized with series, episode, baker, and performance details like technical rankings and Star Baker awards. It contains `r nrow(final_df)` rows and `r ncol(final_df)`columns. This will allow us to analyze patterns, such as consistent winners or surprises in the results.

### Star Baker or Winner from Seasons 5 to 10

We now create a reader-friendly table showing the Star Baker or winner of each episode from Seasons 5 to 10.

```{r}
star_bakers = final_df  |> 
  filter(series >= 5 & series <= 10 & (result == "STAR BAKER" |result == "WINNER"))  |> 
  select(series, episode, baker, result)  |> 
  arrange(series, episode)

knitr::kable(star_bakers, caption = "Star Baker or Winner from Seasons 5 to 10")
```

+ Comment on the table “Star Baker or Winner from Seasons 5 to 10”:

There are some predictable overall winners.

Predictable Winners:

**Nadiya (Season 6)**: Nadiya’s win in Season 6 was highly predictable. She was awarded Star Baker 4 times, including 3 of the last 5 episodes, which showed her strong and consistent performance leading up to her victory.

**Candice (Season 7)**: Candice won Star Baker 3 times, including the final episode, which made her win somewhat predictable, especially given her consistent performance in the second half of the season.

**Sophie (Season 8)**: Sophie’s win also appears predictable, with 2 Star Baker awards in the second half of Season 8, reflecting her strong final push to victory.

**Rahul (Season 9)**: Rahul was Star Baker 2 times and won the last episode, suggesting a strong overall performance leading to his win.

But there are also some surprising results.

**Nancy (Season 5)**: Richard won Star Baker 5 times, yet Nancy won the season. This was a surprising result, as Richard had the strongest mid-season performance, but Nancy’s final push secured her the win.

**David (Season 10)**: David’s win in Season 10 is the biggest surprise. Steph was the Star Baker 4 times, including three consecutive wins in the middle of the season, making her the likely favorite. However, David, who never won Star Baker during the season, won the final.

### Import and Clean Viewership Data

Now, we import and clean the viewers.csv dataset to analyze the show’s viewership over time.

```{r}
viewers_df =
  readr::read_csv(
    "./gbb_datasets/viewers.csv",
    na = c("NA", ".", ""))  |> 
  janitor::clean_names()

head(viewers_df, 10)
```

We calculate the average viewership for Season 1 and Season 5.

```{r}
avg_viewership_season1 = viewers_df  |> 
  pull(series_1) |> 
  mean(na.rm = TRUE)

avg_viewership_season5 = viewers_df  |> 
  pull(series_5) |> 
  mean(na.rm = TRUE)

avg_viewership_season1
avg_viewership_season5
```


The average viewership for Season 1 was `r avg_viewership_season1`, while for Season 5, it was `r avg_viewership_season5`. The show’s viewership appears to have grown significantly as it gained popularity over the seasons.